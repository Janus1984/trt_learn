rm -rf ./*.so ./*.o ./*.d ./*.trt
/usr/local/cuda/bin/nvcc -w -std=c++14 -O3 -UDEBUG -Xcompiler -fPIC -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -I. -I/usr/local/cuda/include -o LayerNormPlugin.o -c LayerNormPlugin.cu
/usr/local/cuda/bin/nvcc -w -std=c++14 -O3 -UDEBUG -Xcompiler -fPIC -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -shared -L/usr/local/cuda/lib64 -lcudart -lcublas -lcublasLt -L/usr/lib/x86_64-linux-gnu/lib -lnvinfer -o LayerNorm.so LayerNormPlugin.o
finish encoder onnx-graphsurgeon!
  61 LayerNormPlugin
&&&& RUNNING TensorRT.trtexec [TensorRT v8203] # ./tensorrt/bin/trtexec --onnx=/workspace/encoder-new.onnx --saveEngine=encoder.plan --minShapes=speech:1x1x80,speech_lengths:1 --optShapes=speech:64x64x80,speech_lengths:64 --maxShapes=speech:256x256x80,speech_lengths:256 --workspace=8192 --plugins=./LayerNormPlugin/LayerNorm.so
[05/10/2022-02:43:00] [I] === Model Options ===
[05/10/2022-02:43:00] [I] Format: ONNX
[05/10/2022-02:43:00] [I] Model: /workspace/encoder-new.onnx
[05/10/2022-02:43:00] [I] Output:
[05/10/2022-02:43:00] [I] === Build Options ===
[05/10/2022-02:43:00] [I] Max batch: explicit batch
[05/10/2022-02:43:00] [I] Workspace: 8192 MiB
[05/10/2022-02:43:00] [I] minTiming: 1
[05/10/2022-02:43:00] [I] avgTiming: 8
[05/10/2022-02:43:00] [I] Precision: FP32
[05/10/2022-02:43:00] [I] Calibration: 
[05/10/2022-02:43:00] [I] Refit: Disabled
[05/10/2022-02:43:00] [I] Sparsity: Disabled
[05/10/2022-02:43:00] [I] Safe mode: Disabled
[05/10/2022-02:43:00] [I] DirectIO mode: Disabled
[05/10/2022-02:43:00] [I] Restricted mode: Disabled
[05/10/2022-02:43:00] [I] Save engine: encoder.plan
[05/10/2022-02:43:00] [I] Load engine: 
[05/10/2022-02:43:00] [I] Profiling verbosity: 0
[05/10/2022-02:43:00] [I] Tactic sources: Using default tactic sources
[05/10/2022-02:43:00] [I] timingCacheMode: local
[05/10/2022-02:43:00] [I] timingCacheFile: 
[05/10/2022-02:43:00] [I] Input(s)s format: fp32:CHW
[05/10/2022-02:43:00] [I] Output(s)s format: fp32:CHW
[05/10/2022-02:43:00] [I] Input build shape: speech_lengths=1+64+256
[05/10/2022-02:43:00] [I] Input build shape: speech=1x1x80+64x64x80+256x256x80
[05/10/2022-02:43:00] [I] Input calibration shapes: model
[05/10/2022-02:43:00] [I] === System Options ===
[05/10/2022-02:43:00] [I] Device: 0
[05/10/2022-02:43:00] [I] DLACore: 
[05/10/2022-02:43:00] [I] Plugins: ./LayerNormPlugin/LayerNorm.so
[05/10/2022-02:43:00] [I] === Inference Options ===
[05/10/2022-02:43:00] [I] Batch: Explicit
[05/10/2022-02:43:00] [I] Input inference shape: speech=64x64x80
[05/10/2022-02:43:00] [I] Input inference shape: speech_lengths=64
[05/10/2022-02:43:00] [I] Iterations: 10
[05/10/2022-02:43:00] [I] Duration: 3s (+ 200ms warm up)
[05/10/2022-02:43:00] [I] Sleep time: 0ms
[05/10/2022-02:43:00] [I] Idle time: 0ms
[05/10/2022-02:43:00] [I] Streams: 1
[05/10/2022-02:43:00] [I] ExposeDMA: Disabled
[05/10/2022-02:43:00] [I] Data transfers: Enabled
[05/10/2022-02:43:00] [I] Spin-wait: Disabled
[05/10/2022-02:43:00] [I] Multithreading: Disabled
[05/10/2022-02:43:00] [I] CUDA Graph: Disabled
[05/10/2022-02:43:00] [I] Separate profiling: Disabled
[05/10/2022-02:43:00] [I] Time Deserialize: Disabled
[05/10/2022-02:43:00] [I] Time Refit: Disabled
[05/10/2022-02:43:00] [I] Skip inference: Disabled
[05/10/2022-02:43:00] [I] Inputs:
[05/10/2022-02:43:00] [I] === Reporting Options ===
[05/10/2022-02:43:00] [I] Verbose: Disabled
[05/10/2022-02:43:00] [I] Averages: 10 inferences
[05/10/2022-02:43:00] [I] Percentile: 99
[05/10/2022-02:43:00] [I] Dump refittable layers:Disabled
[05/10/2022-02:43:00] [I] Dump output: Disabled
[05/10/2022-02:43:00] [I] Profile: Disabled
[05/10/2022-02:43:00] [I] Export timing to JSON file: 
[05/10/2022-02:43:00] [I] Export output to JSON file: 
[05/10/2022-02:43:00] [I] Export profile to JSON file: 
[05/10/2022-02:43:00] [I] 
[05/10/2022-02:43:00] [I] === Device Information ===
[05/10/2022-02:43:00] [I] Selected Device: NVIDIA GeForce GTX 1060
[05/10/2022-02:43:00] [I] Compute Capability: 6.1
[05/10/2022-02:43:00] [I] SMs: 10
[05/10/2022-02:43:00] [I] Compute Clock Rate: 1.6705 GHz
[05/10/2022-02:43:00] [I] Device Global Memory: 6078 MiB
[05/10/2022-02:43:00] [I] Shared Memory per SM: 96 KiB
[05/10/2022-02:43:00] [I] Memory Bus Width: 192 bits (ECC disabled)
[05/10/2022-02:43:00] [I] Memory Clock Rate: 4.004 GHz
[05/10/2022-02:43:00] [I] 
[05/10/2022-02:43:00] [I] TensorRT version: 8.2.3
[05/10/2022-02:43:00] [I] Loading supplied plugin library: ./LayerNormPlugin/LayerNorm.so
[05/10/2022-02:43:01] [I] [TRT] [MemUsageChange] Init CUDA: CPU +177, GPU +0, now: CPU 189, GPU 141 (MiB)
[05/10/2022-02:43:01] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 189 MiB, GPU 141 MiB
[05/10/2022-02:43:02] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 252 MiB, GPU 141 MiB
[05/10/2022-02:43:02] [I] Start parsing network model
[05/10/2022-02:43:02] [I] [TRT] ----------------------------------------------------------------
[05/10/2022-02:43:02] [I] [TRT] Input filename:   /workspace/encoder-new.onnx
[05/10/2022-02:43:02] [I] [TRT] ONNX IR version:  0.0.8
[05/10/2022-02:43:02] [I] [TRT] Opset version:    13
[05/10/2022-02:43:02] [I] [TRT] Producer name:    
[05/10/2022-02:43:02] [I] [TRT] Producer version: 
[05/10/2022-02:43:02] [I] [TRT] Domain:           
[05/10/2022-02:43:02] [I] [TRT] Model version:    0
[05/10/2022-02:43:02] [I] [TRT] Doc string:       
[05/10/2022-02:43:02] [I] [TRT] ----------------------------------------------------------------
[05/10/2022-02:43:32] [I] Finish parsing network model
[05/10/2022-02:43:37] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +270, GPU +110, now: CPU 679, GPU 255 (MiB)
[05/10/2022-02:43:37] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +112, GPU +48, now: CPU 791, GPU 303 (MiB)
[05/10/2022-02:43:37] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.
[05/10/2022-02:46:33] [I] [TRT] Detected 2 inputs and 5 output network tensors.
[05/10/2022-02:46:40] [I] [TRT] Total Host Persistent Memory: 88208
[05/10/2022-02:46:40] [I] [TRT] Total Device Persistent Memory: 9486336
[05/10/2022-02:46:40] [I] [TRT] Total Scratch Memory: 3561555968
[05/10/2022-02:46:40] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2 MiB, GPU 4554 MiB
[05/10/2022-02:46:40] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 135.277ms to assign 12 blocks to 238 nodes requiring 4222178304 bytes.
[05/10/2022-02:46:40] [I] [TRT] Total Activation Memory: 4222178304
[05/10/2022-02:46:40] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 1280, GPU 663 (MiB)
[05/10/2022-02:46:40] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1280, GPU 671 (MiB)
[05/10/2022-02:46:40] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +139, now: CPU 0, GPU 139 (MiB)
[05/10/2022-02:46:40] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1412, GPU 461 (MiB)
[05/10/2022-02:46:40] [I] [TRT] Loaded engine size: 316 MiB
[05/10/2022-02:46:40] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 1594, GPU 611 (MiB)
[05/10/2022-02:46:40] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1594, GPU 619 (MiB)
[05/10/2022-02:46:40] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +138, now: CPU 0, GPU 138 (MiB)
[05/10/2022-02:46:41] [I] Engine built in 220.491 sec.
[05/10/2022-02:46:41] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 1070, GPU 611 (MiB)
[05/10/2022-02:46:41] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1070, GPU 619 (MiB)
[05/10/2022-02:46:44] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +4036, now: CPU 0, GPU 4174 (MiB)
[05/10/2022-02:46:44] [I] Using random values for input speech
[05/10/2022-02:46:44] [I] Created input binding for speech with dimensions 64x64x80
[05/10/2022-02:46:44] [I] Using random values for input speech_lengths
[05/10/2022-02:46:44] [I] Created input binding for speech_lengths with dimensions 64
[05/10/2022-02:46:44] [I] Using random values for output encoder_out_lens
[05/10/2022-02:46:44] [I] Created output binding for encoder_out_lens with dimensions 64
[05/10/2022-02:46:44] [I] Using random values for output encoder_out
[05/10/2022-02:46:44] [I] Created output binding for encoder_out with dimensions 64x15x256
[05/10/2022-02:46:44] [I] Using random values for output ctc_log_probs
[05/10/2022-02:46:44] [I] Created output binding for ctc_log_probs with dimensions 64x15x4233
[05/10/2022-02:46:44] [I] Using random values for output beam_log_probs
[05/10/2022-02:46:44] [I] Created output binding for beam_log_probs with dimensions 64x15x10
[05/10/2022-02:46:44] [I] Using random values for output beam_log_probs_idx
[05/10/2022-02:46:44] [I] Created output binding for beam_log_probs_idx with dimensions 64x15x10
[05/10/2022-02:46:44] [I] Starting inference
[05/10/2022-02:46:48] [I] Warmup completed 2 queries over 200 ms
[05/10/2022-02:46:48] [I] Timing trace has 23 queries over 3.20989 s
[05/10/2022-02:46:48] [I] 
[05/10/2022-02:46:48] [I] === Trace details ===
[05/10/2022-02:46:48] [I] Trace averages of 10 runs:
[05/10/2022-02:46:48] [I] Average on 10 runs - GPU latency: 138.916 ms - Host latency: 140.705 ms (end to end 152.525 ms, enqueue 138.936 ms)
[05/10/2022-02:46:48] [I] Average on 10 runs - GPU latency: 138.951 ms - Host latency: 140.725 ms (end to end 152.505 ms, enqueue 138.923 ms)
[05/10/2022-02:46:48] [I] 
[05/10/2022-02:46:48] [I] === Performance summary ===
[05/10/2022-02:46:48] [I] Throughput: 7.16536 qps
[05/10/2022-02:46:48] [I] Latency: min = 139.927 ms, max = 142.94 ms, mean = 140.71 ms, median = 140.59 ms, percentile(99%) = 142.94 ms
[05/10/2022-02:46:48] [I] End-to-End Host Latency: min = 151.608 ms, max = 155.297 ms, mean = 152.508 ms, median = 152.403 ms, percentile(99%) = 155.297 ms
[05/10/2022-02:46:48] [I] Enqueue Time: min = 137.883 ms, max = 141.767 ms, mean = 138.932 ms, median = 138.914 ms, percentile(99%) = 141.767 ms
[05/10/2022-02:46:48] [I] H2D Latency: min = 0.123047 ms, max = 0.197083 ms, mean = 0.167914 ms, median = 0.171051 ms, percentile(99%) = 0.197083 ms
[05/10/2022-02:46:48] [I] GPU Compute Time: min = 138.078 ms, max = 141.268 ms, mean = 138.94 ms, median = 138.878 ms, percentile(99%) = 141.268 ms
[05/10/2022-02:46:48] [I] D2H Latency: min = 1.45557 ms, max = 1.90723 ms, mean = 1.6024 ms, median = 1.53882 ms, percentile(99%) = 1.90723 ms
[05/10/2022-02:46:48] [I] Total Host Walltime: 3.20989 s
[05/10/2022-02:46:48] [I] Total GPU Compute Time: 3.19561 s
[05/10/2022-02:46:48] [I] Explanations of the performance metrics are printed in the verbose logs.
[05/10/2022-02:46:48] [I] 
&&&& PASSED TensorRT.trtexec [TensorRT v8203] # ./tensorrt/bin/trtexec --onnx=/workspace/encoder-new.onnx --saveEngine=encoder.plan --minShapes=speech:1x1x80,speech_lengths:1 --optShapes=speech:64x64x80,speech_lengths:64 --maxShapes=speech:256x256x80,speech_lengths:256 --workspace=8192 --plugins=./LayerNormPlugin/LayerNorm.so
&&&& RUNNING TensorRT.trtexec [TensorRT v8203] # ./tensorrt/bin/trtexec --onnx=/workspace/decoder.onnx --saveEngine=decoder.plan --minShapes=encoder_out:1x1x256,encoder_out_lens:1,hyps_pad_sos_eos:1x10x64,hyps_lens_sos:1x10,ctc_score:1x10 --optShapes=encoder_out:16x16x256,encoder_out_lens:16,hyps_pad_sos_eos:16x10x64,hyps_lens_sos:16x10,ctc_score:16x10 --maxShapes=encoder_out:32x64x256,encoder_out_lens:32,hyps_pad_sos_eos:32x10x64,hyps_lens_sos:32x10,ctc_score:32x10 --workspace=8192 --plugins=./LayerNormPlugin/LayerNorm.so
[05/10/2022-02:46:48] [I] === Model Options ===
[05/10/2022-02:46:48] [I] Format: ONNX
[05/10/2022-02:46:48] [I] Model: /workspace/decoder.onnx
[05/10/2022-02:46:48] [I] Output:
[05/10/2022-02:46:48] [I] === Build Options ===
[05/10/2022-02:46:48] [I] Max batch: explicit batch
[05/10/2022-02:46:48] [I] Workspace: 8192 MiB
[05/10/2022-02:46:48] [I] minTiming: 1
[05/10/2022-02:46:48] [I] avgTiming: 8
[05/10/2022-02:46:48] [I] Precision: FP32
[05/10/2022-02:46:48] [I] Calibration: 
[05/10/2022-02:46:48] [I] Refit: Disabled
[05/10/2022-02:46:48] [I] Sparsity: Disabled
[05/10/2022-02:46:48] [I] Safe mode: Disabled
[05/10/2022-02:46:48] [I] DirectIO mode: Disabled
[05/10/2022-02:46:48] [I] Restricted mode: Disabled
[05/10/2022-02:46:48] [I] Save engine: decoder.plan
[05/10/2022-02:46:48] [I] Load engine: 
[05/10/2022-02:46:48] [I] Profiling verbosity: 0
[05/10/2022-02:46:48] [I] Tactic sources: Using default tactic sources
[05/10/2022-02:46:48] [I] timingCacheMode: local
[05/10/2022-02:46:48] [I] timingCacheFile: 
[05/10/2022-02:46:48] [I] Input(s)s format: fp32:CHW
[05/10/2022-02:46:48] [I] Output(s)s format: fp32:CHW
[05/10/2022-02:46:48] [I] Input build shape: ctc_score=1x10+16x10+32x10
[05/10/2022-02:46:48] [I] Input build shape: hyps_lens_sos=1x10+16x10+32x10
[05/10/2022-02:46:48] [I] Input build shape: hyps_pad_sos_eos=1x10x64+16x10x64+32x10x64
[05/10/2022-02:46:48] [I] Input build shape: encoder_out_lens=1+16+32
[05/10/2022-02:46:48] [I] Input build shape: encoder_out=1x1x256+16x16x256+32x64x256
[05/10/2022-02:46:48] [I] Input calibration shapes: model
[05/10/2022-02:46:48] [I] === System Options ===
[05/10/2022-02:46:48] [I] Device: 0
[05/10/2022-02:46:48] [I] DLACore: 
[05/10/2022-02:46:48] [I] Plugins: ./LayerNormPlugin/LayerNorm.so
[05/10/2022-02:46:48] [I] === Inference Options ===
[05/10/2022-02:46:48] [I] Batch: Explicit
[05/10/2022-02:46:48] [I] Input inference shape: encoder_out=16x16x256
[05/10/2022-02:46:48] [I] Input inference shape: encoder_out_lens=16
[05/10/2022-02:46:48] [I] Input inference shape: hyps_pad_sos_eos=16x10x64
[05/10/2022-02:46:48] [I] Input inference shape: hyps_lens_sos=16x10
[05/10/2022-02:46:48] [I] Input inference shape: ctc_score=16x10
[05/10/2022-02:46:48] [I] Iterations: 10
[05/10/2022-02:46:48] [I] Duration: 3s (+ 200ms warm up)
[05/10/2022-02:46:48] [I] Sleep time: 0ms
[05/10/2022-02:46:48] [I] Idle time: 0ms
[05/10/2022-02:46:48] [I] Streams: 1
[05/10/2022-02:46:48] [I] ExposeDMA: Disabled
[05/10/2022-02:46:48] [I] Data transfers: Enabled
[05/10/2022-02:46:48] [I] Spin-wait: Disabled
[05/10/2022-02:46:48] [I] Multithreading: Disabled
[05/10/2022-02:46:48] [I] CUDA Graph: Disabled
[05/10/2022-02:46:48] [I] Separate profiling: Disabled
[05/10/2022-02:46:48] [I] Time Deserialize: Disabled
[05/10/2022-02:46:48] [I] Time Refit: Disabled
[05/10/2022-02:46:48] [I] Skip inference: Disabled
[05/10/2022-02:46:48] [I] Inputs:
[05/10/2022-02:46:48] [I] === Reporting Options ===
[05/10/2022-02:46:48] [I] Verbose: Disabled
[05/10/2022-02:46:48] [I] Averages: 10 inferences
[05/10/2022-02:46:48] [I] Percentile: 99
[05/10/2022-02:46:48] [I] Dump refittable layers:Disabled
[05/10/2022-02:46:48] [I] Dump output: Disabled
[05/10/2022-02:46:48] [I] Profile: Disabled
[05/10/2022-02:46:48] [I] Export timing to JSON file: 
[05/10/2022-02:46:48] [I] Export output to JSON file: 
[05/10/2022-02:46:48] [I] Export profile to JSON file: 
[05/10/2022-02:46:48] [I] 
[05/10/2022-02:46:48] [I] === Device Information ===
[05/10/2022-02:46:48] [I] Selected Device: NVIDIA GeForce GTX 1060
[05/10/2022-02:46:48] [I] Compute Capability: 6.1
[05/10/2022-02:46:48] [I] SMs: 10
[05/10/2022-02:46:48] [I] Compute Clock Rate: 1.6705 GHz
[05/10/2022-02:46:48] [I] Device Global Memory: 6078 MiB
[05/10/2022-02:46:48] [I] Shared Memory per SM: 96 KiB
[05/10/2022-02:46:48] [I] Memory Bus Width: 192 bits (ECC disabled)
[05/10/2022-02:46:48] [I] Memory Clock Rate: 4.004 GHz
[05/10/2022-02:46:48] [I] 
[05/10/2022-02:46:48] [I] TensorRT version: 8.2.3
[05/10/2022-02:46:48] [I] Loading supplied plugin library: ./LayerNormPlugin/LayerNorm.so
[05/10/2022-02:46:49] [I] [TRT] [MemUsageChange] Init CUDA: CPU +177, GPU +0, now: CPU 189, GPU 141 (MiB)
[05/10/2022-02:46:49] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 189 MiB, GPU 141 MiB
[05/10/2022-02:46:49] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 252 MiB, GPU 141 MiB
[05/10/2022-02:46:49] [I] Start parsing network model
[05/10/2022-02:46:49] [I] [TRT] ----------------------------------------------------------------
[05/10/2022-02:46:49] [I] [TRT] Input filename:   /workspace/decoder.onnx
[05/10/2022-02:46:49] [I] [TRT] ONNX IR version:  0.0.7
[05/10/2022-02:46:49] [I] [TRT] Opset version:    13
[05/10/2022-02:46:49] [I] [TRT] Producer name:    pytorch
[05/10/2022-02:46:49] [I] [TRT] Producer version: 1.10
[05/10/2022-02:46:49] [I] [TRT] Domain:           
[05/10/2022-02:46:49] [I] [TRT] Model version:    0
[05/10/2022-02:46:49] [I] [TRT] Doc string:       
[05/10/2022-02:46:49] [I] [TRT] ----------------------------------------------------------------
[05/10/2022-02:47:02] [I] Finish parsing network model
[05/10/2022-02:47:03] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +269, GPU +110, now: CPU 586, GPU 255 (MiB)
[05/10/2022-02:47:03] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +112, GPU +48, now: CPU 698, GPU 303 (MiB)
[05/10/2022-02:47:03] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.
[05/10/2022-02:47:49] [I] [TRT] Detected 5 inputs and 2 output network tensors.
[05/10/2022-02:47:52] [I] [TRT] Total Host Persistent Memory: 208
[05/10/2022-02:47:52] [I] [TRT] Total Device Persistent Memory: 0
[05/10/2022-02:47:52] [I] [TRT] Total Scratch Memory: 1457296896
[05/10/2022-02:47:52] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 2089 MiB
[05/10/2022-02:47:52] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.20629ms to assign 11 blocks to 14 nodes requiring 1457302018 bytes.
[05/10/2022-02:47:52] [I] [TRT] Total Activation Memory: 1457302018
[05/10/2022-02:47:52] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 752, GPU 387 (MiB)
[05/10/2022-02:47:52] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 752, GPU 395 (MiB)
[05/10/2022-02:47:52] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +64, now: CPU 0, GPU 64 (MiB)
[05/10/2022-02:47:52] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 795, GPU 297 (MiB)
[05/10/2022-02:47:52] [I] [TRT] Loaded engine size: 96 MiB
[05/10/2022-02:47:52] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 847, GPU 353 (MiB)
[05/10/2022-02:47:52] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 847, GPU 361 (MiB)
[05/10/2022-02:47:52] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +44, now: CPU 0, GPU 44 (MiB)
[05/10/2022-02:47:52] [I] Engine built in 64.0659 sec.
[05/10/2022-02:47:52] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 635, GPU 353 (MiB)
[05/10/2022-02:47:52] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 635, GPU 361 (MiB)
[05/10/2022-02:47:53] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1390, now: CPU 0, GPU 1434 (MiB)
[05/10/2022-02:47:53] [I] Using random values for input encoder_out
[05/10/2022-02:47:53] [I] Created input binding for encoder_out with dimensions 16x16x256
[05/10/2022-02:47:53] [I] Using random values for input encoder_out_lens
[05/10/2022-02:47:53] [I] Created input binding for encoder_out_lens with dimensions 16
[05/10/2022-02:47:53] [I] Using random values for input hyps_pad_sos_eos
[05/10/2022-02:47:53] [I] Created input binding for hyps_pad_sos_eos with dimensions 16x10x64
[05/10/2022-02:47:53] [I] Using random values for input hyps_lens_sos
[05/10/2022-02:47:53] [I] Created input binding for hyps_lens_sos with dimensions 16x10
[05/10/2022-02:47:53] [I] Using random values for input ctc_score
[05/10/2022-02:47:53] [I] Created input binding for ctc_score with dimensions 16x10
[05/10/2022-02:47:53] [I] Using random values for output decoder_out
[05/10/2022-02:47:53] [I] Created output binding for decoder_out with dimensions 16x10x63x4233
[05/10/2022-02:47:53] [I] Using random values for output best_index
[05/10/2022-02:47:53] [I] Created output binding for best_index with dimensions 16
[05/10/2022-02:47:53] [I] Starting inference
[05/10/2022-02:47:57] [I] Warmup completed 2 queries over 200 ms
[05/10/2022-02:47:57] [I] Timing trace has 29 queries over 3.32759 s
[05/10/2022-02:47:57] [I] 
[05/10/2022-02:47:57] [I] === Trace details ===
[05/10/2022-02:47:57] [I] Trace averages of 10 runs:
[05/10/2022-02:47:57] [I] Average on 10 runs - GPU latency: 113.915 ms - Host latency: 128.148 ms (end to end 130.366 ms, enqueue 113.874 ms)
[05/10/2022-02:47:57] [I] Average on 10 runs - GPU latency: 114.332 ms - Host latency: 128.626 ms (end to end 130.855 ms, enqueue 114.297 ms)
[05/10/2022-02:47:57] [I] 
[05/10/2022-02:47:57] [I] === Performance summary ===
[05/10/2022-02:47:57] [I] Throughput: 8.71502 qps
[05/10/2022-02:47:57] [I] Latency: min = 126.235 ms, max = 130.735 ms, mean = 128.447 ms, median = 128.313 ms, percentile(99%) = 130.735 ms
[05/10/2022-02:47:57] [I] End-to-End Host Latency: min = 128.427 ms, max = 132.985 ms, mean = 130.674 ms, median = 130.538 ms, percentile(99%) = 132.985 ms
[05/10/2022-02:47:57] [I] Enqueue Time: min = 111.953 ms, max = 116.69 ms, mean = 114.143 ms, median = 114.096 ms, percentile(99%) = 116.69 ms
[05/10/2022-02:47:57] [I] H2D Latency: min = 0.0375977 ms, max = 0.0664673 ms, mean = 0.052777 ms, median = 0.0544434 ms, percentile(99%) = 0.0664673 ms
[05/10/2022-02:47:57] [I] GPU Compute Time: min = 112.025 ms, max = 116.689 ms, mean = 114.18 ms, median = 114.131 ms, percentile(99%) = 116.689 ms
[05/10/2022-02:47:57] [I] D2H Latency: min = 13.9111 ms, max = 14.7921 ms, mean = 14.214 ms, median = 14.1251 ms, percentile(99%) = 14.7921 ms
[05/10/2022-02:47:57] [I] Total Host Walltime: 3.32759 s
[05/10/2022-02:47:57] [I] Total GPU Compute Time: 3.31123 s
[05/10/2022-02:47:57] [I] Explanations of the performance metrics are printed in the verbose logs.
[05/10/2022-02:47:57] [I] 
&&&& PASSED TensorRT.trtexec [TensorRT v8203] # ./tensorrt/bin/trtexec --onnx=/workspace/decoder.onnx --saveEngine=decoder.plan --minShapes=encoder_out:1x1x256,encoder_out_lens:1,hyps_pad_sos_eos:1x10x64,hyps_lens_sos:1x10,ctc_score:1x10 --optShapes=encoder_out:16x16x256,encoder_out_lens:16,hyps_pad_sos_eos:16x10x64,hyps_lens_sos:16x10,ctc_score:16x10 --maxShapes=encoder_out:32x64x256,encoder_out_lens:32,hyps_pad_sos_eos:32x10x64,hyps_lens_sos:32x10,ctc_score:32x10 --workspace=8192 --plugins=./LayerNormPlugin/LayerNorm.so
